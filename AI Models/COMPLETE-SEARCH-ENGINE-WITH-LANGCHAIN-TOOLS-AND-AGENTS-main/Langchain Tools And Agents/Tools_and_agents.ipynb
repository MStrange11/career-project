{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65617e43",
   "metadata": {},
   "source": [
    "### ‚úÖ **Wrapper**\n",
    "\n",
    "> A **Wrapper** is like a helper that **talks to a website** (like Arxiv or Wikipedia) and **gets the data**.\n",
    "\n",
    "üß† Think: \"Go and get the info!\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **QueryRun**\n",
    "\n",
    "> A **QueryRun** is a tool that **asks a question** and uses the wrapper to **get the answer**.\n",
    "\n",
    "üß† Think: \"Ask and get the answer back!\"\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Real-Life Example:\n",
    "\n",
    "* **Wrapper** = goes to Arxiv and brings research papers.\n",
    "* **QueryRun** = asks: ‚ÄúWhat are the latest ML papers?‚Äù and shows the results using the wrapper.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Summary:\n",
    "\n",
    "| Name         | What it does                             |\n",
    "| ------------ | ---------------------------------------- |\n",
    "| **Wrapper**  | Gets data from a website                 |\n",
    "| **QueryRun** | Uses the wrapper to answer your question |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46d11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Arxiv----- research papers\n",
    "#Tools creation \n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82583b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating wikipedia_wrapper\n",
    "wikipedia_wrapper= WikipediaAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=250\n",
    ")\n",
    "\n",
    "#Creating wikipedia tool using wikipedia_wrapper which is inbuilt\n",
    "wikipedia_tool= WikipediaQueryRun(api_wrapper=wikipedia_wrapper)\n",
    "wikipedia_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be38f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly creating arxiv_wrapper and arxiv_tool\n",
    "\n",
    "arxiv_wrapper= ArxivAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=250\n",
    ")\n",
    "\n",
    "arxiv_tool= ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71367f",
   "metadata": {},
   "source": [
    "# Combining tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b76089",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools= [wikipedia_tool,arxiv_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b11ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d17a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001915449B880>, search_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader= WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "docs= loader.load()\n",
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "texts= text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore= FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "\n",
    "retriever= vectorstore.as_retriever()\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab344f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating own retriever tool using the retriever \n",
    "# Convert reteriever to a tool,so that it can used as a tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"langsmith-search\",\n",
    "    description=\"Useful for retrieving information from LangChain documentation.\",\n",
    ")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c738fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia arxiv langsmith-search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'd:\\\\GENERATIVE AI\\\\Langchain projects\\\\venv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " Tool(name='langsmith-search', description='Useful for retrieving information from LangChain documentation.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001917EBA95A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001915449B880>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001917EBA93F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001915449B880>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools= [wikipedia_tool,arxiv_tool,retriever_tool]\n",
    "print(tools[0].name, tools[1].name, tools[2].name)\n",
    "tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777471e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000019156B04730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000019156C4C100>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run all the tools with Agents and LLM Models\n",
    "\n",
    "## Tools,LLM---->AgentExecutor\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "import os \n",
    "\n",
    "groqApiKey= os.getenv(\"GROQ_API_KEY\")\n",
    "openai.api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groqApiKey, model=\"Llama3-8b-8192\")\n",
    "llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c728fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt template using hub \n",
    "from langchain import hub \n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c14da35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000191602DCCA0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000191602DCCA0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000019156B04730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000019156C4C100>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Useful for retrieving information from LangChain documentation.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents \n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm=llm,tools=tools,prompt=prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5305d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For running the agent, we can use AgentExecutor\n",
    "from langchain.agents import AgentExecutor \n",
    "agent_executor= AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72ec11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'LangChain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of languag\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'langchain integration framework'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceChangelogCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observability‚Äã\n",
      "\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observability‚Äã\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Get started by creating your first prompt.\n",
      "Iterate on models and prompts using the Playground.\n",
      "Manage prompts programmatically in your application.\n",
      "Was this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\u001b[0m\u001b[32;1m\u001b[1;3mThe information provided about LangChain has been helpful in understanding what LangChain is. LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is LangChain?',\n",
       " 'output': 'The information provided about LangChain has been helpful in understanding what LangChain is. LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0912d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Machine learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Machine learning\n",
      "Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks wit\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that the tool yielded a result, and I can continue the conversation without using any more tools.\n",
      "\n",
      "Machine learning is a subfield of artificial intelligence that involves the use of algorithms and statistical models to enable machines to learn from data and make predictions or decisions. The goal of machine learning is to develop systems that can automatically improve their performance on a task over time, without being explicitly programmed. This is achieved by training the system on a dataset, which allows it to learn patterns and relationships in the data.\n",
      "\n",
      "Some common applications of machine learning include image and speech recognition, natural language processing, recommender systems, and predictive modeling. Machine learning has many potential uses, including healthcare, finance, and education, and is a rapidly growing field with many new developments and applications being explored.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Machine learning?',\n",
       " 'output': 'It seems that the tool yielded a result, and I can continue the conversation without using any more tools.\\n\\nMachine learning is a subfield of artificial intelligence that involves the use of algorithms and statistical models to enable machines to learn from data and make predictions or decisions. The goal of machine learning is to develop systems that can automatically improve their performance on a task over time, without being explicitly programmed. This is achieved by training the system on a dataset, which allows it to learn patterns and relationships in the data.\\n\\nSome common applications of machine learning include image and speech recognition, natural language processing, recommender systems, and predictive modeling. Machine learning has many potential uses, including healthcare, finance, and education, and is a rapidly growing field with many new developments and applications being explored.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb5a4db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on c\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Attention Is All You Need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer,\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Attention Is All You Need\" by Vaswani et al. introduced the transformer architecture, which has since become a widely used and influential model in natural language processing and other areas of machine learning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about the paper arXiv:1706.03762 ?',\n",
       " 'output': 'The paper \"Attention Is All You Need\" by Vaswani et al. introduced the transformer architecture, which has since become a widely used and influential model in natural language processing and other areas of machine learning.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"tell me about the paper arXiv:1706.03762 ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c887d",
   "metadata": {},
   "source": [
    " **definitions**:\n",
    "\n",
    "# Tool in LangChain:\n",
    "\n",
    "A **Tool** is a function that does a specific task, like doing math, searching the web, or running code. It helps the chatbot get things done.\n",
    "\n",
    "# Agent in LangChain:\n",
    "\n",
    "An **Agent** is a smart controller that decides which tool to use and in what order to answer a user's question.\n",
    "\n",
    "\n",
    "Here's a **real-world scenario** with a clear example of a **Tool** and an **Agent**:\n",
    "\n",
    "\n",
    "##  **Scenario: Planning a Trip**\n",
    "\n",
    "You ask a chatbot:\n",
    "\n",
    "> \"Book a flight from New York to Los Angeles, check the weather there, and suggest what to pack.\"\n",
    "\n",
    "\n",
    "###  **Tools Involved**:\n",
    "\n",
    "1.  **Flight Booking Tool** ‚Äì Books a flight between two cities.\n",
    "2.  **Weather API Tool** ‚Äì Checks weather in Los Angeles.\n",
    "3.  **Packing Suggestion Tool** ‚Äì Suggests what to pack based on weather.\n",
    "\n",
    "Each of these is a **Tool** ‚Äì they do one specific job.\n",
    "\n",
    "\n",
    "###  **Agent's Role**:\n",
    "\n",
    "The **Agent** will:\n",
    "\n",
    "1. Understand your request.\n",
    "2. Use the **Flight Booking Tool** to book the flight.\n",
    "3. Use the **Weather Tool** to check LA‚Äôs weather.\n",
    "4. Use the **Packing Tool** to suggest clothes.\n",
    "5. Combine all results and give you one complete answer.\n",
    "\n",
    "\n",
    "###  **Agent in Action:**\n",
    "\n",
    "Here‚Äôs how it thinks step-by-step:\n",
    "\n",
    ">  ‚ÄúI need to book a flight‚Äù ‚Üí Use **Flight Tool**\n",
    ">  ‚ÄúI need the weather in LA‚Äù ‚Üí Use **Weather Tool**\n",
    ">  ‚ÄúI need to suggest what to pack‚Äù ‚Üí Use **Packing Tool**\n",
    ">  Then combine everything into one smart reply!\n",
    "\n",
    "\n",
    "### **Final Answer from the Agent**:\n",
    "\n",
    "> \"Your flight from New York to Los Angeles is booked for May 30. The weather in LA is sunny with highs of 28¬∞C. You should pack light clothes, sunglasses, and sunscreen.\"\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Role      | Example                                                                       |\n",
    "| --------- | ----------------------------------------------------------------------------- |\n",
    "| **Tool**  | Does one job (e.g., checks weather)                                           |\n",
    "| **Agent** | Understands the full task, uses tools in the right order, and replies smartly |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
