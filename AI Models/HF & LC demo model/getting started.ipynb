{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf3935-b50a-4777-90c2-092a72e26670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59885ec5-bd74-4733-ad29-4f9e0474757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36577dc2-e5e0-4419-8615-2c1f4cd9248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb4a659-ee23-45a0-89c4-f31d46f606a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "secret_key = os.getenv('HF_TOKEN')\n",
    "# print(secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d3b91-8da0-48b0-ade4-da2b55627e50",
   "metadata": {},
   "source": [
    "# testing API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d48890-bd98-4c42-b9d0-953ff07b575c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning (ML) is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
      "\n",
      "The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically to achieve their design goals.\n",
      "\n",
      "There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. Each type has its own approach to learning from data.\n",
      "\n",
      "1. Supervised learning: The algorithm is provided with labeled data, which means the correct answer is given. The goal is to learn a general rule that maps inputs to outputs based on example input-output pairs.\n",
      "\n",
      "2. Un\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(token=secret_key)\n",
    "response = client.text_generation(\n",
    "    \"[INST] what is machine learning [/INST]\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_new_tokens=200\n",
    ")\n",
    "print(response)  # Check raw output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc4536-1fee-45f6-808a-5a5a8fc7d1d7",
   "metadata": {},
   "source": [
    "# Best way to access huggingface model is endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23240d1-1f18-4d81-9f89-f24d99e25ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f23640-cad1-44c7-b61a-a45b4ae05bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\" # model api\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=200,  # <-- Moved OUT of model_kwargs\n",
    "    huggingfacehub_api_token=secret_key,  # <-- API token as direct parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9616736-275e-4162-8c5d-5366d2ad4e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with the system receiving input data and creating a model based on that data. The model is then used to make predictions or decisions without being specifically programmed to perform the task. Machine learning algorithms are categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.\\n\\nSupervised learning uses labeled data to train the model, while unsupervised learning works with unlabeled data to find patterns and structure. Reinforcement learning involves an agent learning to make decisions by taking actions in an environment to achieve a goal.\\n\\nMachine learning is used in various applications, such as image recognition, speech recognition, natural language processing, and predictive analytics, among others. It plays'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"what is machine learning\"\n",
    "llm.invoke(\"[INST]\" + msg+ \"[INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473432a4-ce76-4a25-8ad2-b3bec764844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugging Face is a technology company specializing in natural language processing (NLP). They are well-known for their open-source library called Transformers, which provides thousands of pre-trained models for a wide range of NLP tasks such as translation, summarization, and sentiment analysis. These models are based on deep learning techniques and are trained on large datasets. Hugging Face also offers a platform for building and deploying custom NLP models.\\n\\nLangchain is not a widely recognized or established term in the field of NLP or AI. It's possible that it could refer to a specific project, tool, or company in the context of a specific community or organization, but without more context, it's difficult to provide a precise definition. If you have more information about Langchain, I'd be happy to help further!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"what is huggingface and langchain\"\n",
    "llm.invoke(\"[INST]\" + msg+ \"[INST]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a7285-1e78-4c3d-8c3d-eace1c1bbdb4",
   "metadata": {},
   "source": [
    "# Creating custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c372573a-9896-48ad-9df1-1fbc1305408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf255d0-7be6-4063-925c-b986d200d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \"Who won the cricket world cup in 2011?\"\n",
    "\n",
    "template = \"\"\" Question:{ques}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['ques'])\n",
    "chain = prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99e2f4d1-fbf1-4de4-91bc-06c5d7b3f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The Cricket World Cup is an international cricket tournament that takes place every four years.\n",
      "2. The 2011 Cricket World Cup was held in India, Sri Lanka, and Bangladesh.\n",
      "3. The final match of the 2011 Cricket World Cup was played between India and Sri Lanka.\n",
      "4. The winner of the 2011 Cricket World Cup was India, as they defeated Sri Lanka in the final match.\n",
      "\n",
      "So, India won the cricket world cup in 2011.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(ques))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7750f3-4556-41d0-8691-30c709ad09c8",
   "metadata": {},
   "source": [
    "# Huggingface Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d51fee-11d8-4d31-99b9-77e42cbe2ded",
   "metadata": {},
   "source": [
    "#### note when doing first time, this will download the data from huggingface and store in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fbd6e51-1b47-4ac5-a3e7-141997992bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b8b2aa4-7856-4c94-b96f-dd3651055a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6288581c1ebb457fabb5d925b29fae09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jynts\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jynts\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbde057687d4b9183994253745eda56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d29d320f79444abc0d4ad632b6d41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe548b449b244bea920fe529b87eeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932a8ba842254381b252f8b32e840357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89de2c41ee884e8796767362a5ae212d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2e7a1784ce40f9b7868149def85f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d3dda7-3e98-4b84-b050-bb1cd3a67210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, max_new_tokens=200)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea12db7a-075d-4b74-a4ec-dc29a609b5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is gen AI?\n",
      "\n",
      "Gen AI: The concept of AI is to create something different from us. In this case, to create something that you don't know you can control. You can make the computer understand how to behave.\n",
      "\n",
      "Let's say we are building a robot. It is an intelligent robot, which is a human. It has been programmed to act like a human. It is programmed to do things that we don't expect. Now, we can imagine what it will do, but in the future, it would have been programmed to act like a normal human.\n",
      "\n",
      "What does that mean?\n",
      "\n",
      "The difference between gen AI and the human-like AI, is that it has to know that it is capable of doing something it does not know it can do. Gen AI is like a machine learning system which can be programmed to do things that it does not see it can do.\n",
      "\n",
      "So, what can we do?\n",
      "\n",
      "We can create a machine learning system which will\n"
     ]
    }
   ],
   "source": [
    "res = hf.invoke(\"what is gen AI?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10dba221-ae4b-4ad8-8bc5-44b46ee2a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "## Use HuggingfacePipelines With Gpu\n",
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    # device=0, # replace with device_map=\"auto\" to use the accelerate library. [-1 for cpu and 0 for gpu]\n",
    "    device_map=\"auto\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "733d73a1-c5f2-40fb-b00c-6df5a477c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\" Question:{ques}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt|gpu_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3a9d5cd-ff86-4363-a81d-457024e288d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question:what knowledge is required to become a AI specialist?\n",
      "Answer: Let's think step by step. Think about the various types of AI that we are going to be able to accomplish. We're going to be able to create, train, and eventually eventually create robots that can perform some tasks within a particular domain. We're going to be able to create AI that will perform tasks that we don't want, that we don't want to do. However, our ability to do these things will depend on how far we've come in terms of the amount of information that we can come up with.\n"
     ]
    }
   ],
   "source": [
    "ques = \"what knowledge is required to become a AI specialist?\"\n",
    "res = chain.invoke({\"ques\":ques})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc7dc2-32de-44a1-9367-095258ff38fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
